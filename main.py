from flask import Flask, jsonify, send_file
- import traceback
- import requests
import os
import logging
import json
from datetime import datetime
import pandas as pd
import numpy as np
import joblib
import xgboost as xgb
import schedule
import threading
import time
import sys
import subprocess
import io
import base64

# Standard Library Imports first, then third-party, then local
# (Order is generally good, just consolidated)

# === Auto-install and Import Core Dependencies ===

# Requests (Imported once at the top, auto-install if missing)
try:
    import requests
except ImportError:
    print("Requests library not found, attempting to install...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "requests"])
    import requests
    print("Requests library installed and imported.")

# Matplotlib (Imported once at the top, auto-install if missing)
try:
    import matplotlib
    matplotlib.use('Agg') # Use Agg backend for non-interactive plotting in threads
    import matplotlib.pyplot as plt
    matplotlib.rcParams['font.family'] = 'Arial' # Set default font
except ImportError:
    print("Matplotlib not found, attempting to install...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "matplotlib"])
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    matplotlib.rcParams['font.family'] = 'Arial'
    print("Matplotlib installed and imported.")

# stable-baselines3 and gym
try:
    import gym
    import stable_baselines3

    print("Gym, Stable-Baselines3, and Shimmy are already installed.")
except ImportError:
    print("stable-baselines3, gym, or shimmy not found, attempting to install all...")
    # Ensure 'subprocess' and 'sys' are imported at the very top of your file
    subprocess.check_call([sys.executable, "-m", "pip", "install", "gym", "stable-baselines3", "shimmy~=2.0"])
    import gym
    import stable_baselines3
    print("gym, stable-baselines3, and shimmy installed and imported successfully.")
    
- from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv # ‡∏≠‡∏≤‡∏à‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ñ‡πâ‡∏≤ PIMMAEnv ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô VecEnv ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
- from gym import Env, spaces

# SHAP
try:
    import shap
    print("SHAP is already installed.")
except ImportError:
    print("SHAP library not found, attempting to install...")
    # SHAP can sometimes have specific dependencies, simple install first
    subprocess.check_call([sys.executable, "-m", "pip", "install", "shap"])
    import shap
    print("SHAP library installed and imported successfully.")
    
# === Global Configuration & Initialization ===
# Create necessary folders
os.makedirs("logs", exist_ok=True)
os.makedirs("models", exist_ok=True)
os.makedirs("presets", exist_ok=True) # Ensure presets folder exists
baseline_mean = None
explainer = None

# Logging Configuration
logging.basicConfig(
    filename="logs/ai_service.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# Flask App Initialization
app = Flask(__name__)

# --- Constants ---
MODEL_PATH = "models/trained_model.xgb"
BEST_MODEL_PATH = "models/best_model.xgb"
TRADE_DATA_LOG_FILE = "logs/trade_data.csv"
LAST_RETRAIN_JSON = "logs/last_retrain.json"
WFT_RESULTS_CSV = "logs/walking_forward_results.csv"
EXECUTION_FAILURES_CSV = "logs/execution_failures.csv"
EXIT_DECISIONS_CSV = "logs/exit_decisions.csv"
AI_FAIL_ENTRY_CSV = "logs/ai_fail_entry.csv"

# --- Global Variables ---
USE_FAKE_MODEL = os.environ.get("USE_FAKE_MODEL", "False").lower() == "true" # Control via env variable
model = None # Will hold the loaded XGBoost model
ENFORCE_LICENSE = False # Toggle for license enforcement
ALLOWED_LICENSE_KEYS = {"ABC123", "YOUR_VALID_KEY_HERE"}

# === Utility Functions ===
def parse_feature_string(feature_str):
    """Parses a comma-separated string of features into a list of floats."""
    if not feature_str or not isinstance(feature_str, str):
        return []
    return [float(f) for f in feature_str.strip("[]").split(',') if f.strip()]

def _save_retrain_status(status, samples_used, duration_seconds, file_path=LAST_RETRAIN_JSON):
    """Saves the retraining status to a JSON file."""
    data = {
        "timestamp": datetime.now().isoformat(),
        "samples_used": int(samples_used),
        "duration_seconds": float(duration_seconds),
        "status": str(status)
    }
    try:
        with open(file_path, "w") as f:
            json.dump(data, f, indent=2)
        logging.info(f"[RETRAIN_STATUS_SAVE] Status saved: {status}, Samples: {samples_used}, Duration: {duration_seconds}s")
    except IOError as e:
        logging.error(f"[RETRAIN_STATUS_SAVE] Error saving retrain status to {file_path}: {e}", exc_info=True)

# === Reinforcement Learning (RL) Setup ===

def _initialize_shap_explainer():
    global model, explainer # ‡πÉ‡∏ä‡πâ model ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô global XGBoost model ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
    if model is not None and isinstance(model, xgb.XGBModel): # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô XGBoost model
        try:
            explainer = shap.TreeExplainer(model)
            logging.info("[SHAP_INIT] SHAP TreeExplainer initialized successfully for the XGBoost model.")
        except Exception as e:
            logging.error(f"[SHAP_INIT] Failed to initialize SHAP TreeExplainer: {e}", exc_info=True)
            explainer = None
    elif model is not None:
        logging.warning(f"[SHAP_INIT] Model type ({type(model)}) is not directly XGBModel. SHAP TreeExplainer might not be optimal or work. Consider other SHAP explainers if needed.")
        # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ KernelExplainer ‡∏´‡∏£‡∏∑‡∏≠ DeepExplainer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∑‡πà‡∏ô
        # ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô None ‡∏ñ‡πâ‡∏≤ TreeExplainer ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        explainer = None # ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡∏•‡∏≠‡∏á explainer = shap.Explainer(model) ‡∏ñ‡πâ‡∏≤ SHAP ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    else:
        explainer = None
        logging.info("[SHAP_INIT] Main model not loaded. SHAP explainer not initialized.")


MODEL_FOLDER = "models" # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
os.makedirs(MODEL_FOLDER, exist_ok=True) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå models ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ

# ‡∏™‡∏£‡πâ‡∏≤‡∏á environment ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
class PIMMAEnv(Env):
    def __init__(self, feature_dim):
        super().__init__()
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(feature_dim,), dtype=np.float32)
        self.action_space = spaces.Discrete(3)  # 0=hold, 1=buy, 2=sell
        self.current_obs = np.zeros(self.observation_space.shape, dtype=np.float32) # Ensure float32
        self._current_step = 0
        self._max_steps = 200 # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô step ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠ episode

    def reset(self):
        self.current_obs = np.zeros(self.observation_space.shape, dtype=np.float32)
        self._current_step = 0
        return self.current_obs

    def step(self, action):
        # ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï self.current_obs ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏ï‡∏•‡∏≤‡∏î
        # ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì reward ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ action ‡∏ô‡∏±‡πâ‡∏ô‡πÜ
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÉ‡∏ä‡πâ dummy reward ‡πÅ‡∏•‡∏∞ next_obs ‡∏Ñ‡∏∑‡∏≠ obs ‡πÄ‡∏î‡∏¥‡∏°
        # ‡πÅ‡∏•‡∏∞ done ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô True ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏∂‡∏á _max_steps (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ learn() ‡∏à‡∏ö‡πÑ‡∏î‡πâ)
        self._current_step += 1
        reward = np.random.rand() - 0.5 # Dummy reward: random -0.5 to 0.5
        
        # ‡∏ñ‡πâ‡∏≤ MQL ‡∏™‡πà‡∏á next_obs ‡∏°‡∏≤‡πÉ‡∏ô /rl/store, ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï current_obs ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ô‡∏±‡∏Å
        # ‡πÅ‡∏ï‡πà PPO.learn() ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å step() ‡∏ô‡∏µ‡πâ‡∏ã‡πâ‡∏≥‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö rollouts
        # ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô PIMMAEnv ‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ simulate environment ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á
        # ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏¥‡∏ò‡∏µ map ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å rl_buffer ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô PPO.learn() ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏ã‡∏∂‡πà‡∏á‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤)

        # Placeholder: ‡∏™‡∏£‡πâ‡∏≤‡∏á next_obs ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏à‡∏≤‡∏Å current_obs
        # ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á next_obs ‡∏Ñ‡∏ß‡∏£‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å action
        next_obs = self.current_obs + np.random.normal(0, 0.1, self.current_obs.shape).astype(np.float32)
        self.current_obs = next_obs # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï current_obs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö step ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

        done = self._current_step >= self._max_steps
        info = {}
        return self.current_obs, reward, done, info

    def render(self, mode='human'): # Optional
        pass

    def close(self): # Optional
        pass

# ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á policy
feature_dim = 45  # ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
rl_model_path = os.path.join(MODEL_FOLDER, "rl_model.zip")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á VecEnv ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ PPO
# PPO ‡πÉ‡∏ô stable-baselines3 ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö VecEnv
# ‡∏ñ‡πâ‡∏≤ PIMMAEnv ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á wrap ‡∏î‡πâ‡∏ß‡∏¢ DummyVecEnv ‡∏´‡∏£‡∏∑‡∏≠ SubprocVecEnv
# ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤ PIMMAEnv ‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ PPO.learn() ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏Å‡πá‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á wrap ‡∏ï‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
# ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ VecEnv ‡πÄ‡∏õ‡πá‡∏ô practice ‡∏ó‡∏µ‡πà‡∏î‡∏µ
# rl_env_instance = PIMMAEnv(feature_dim)
# rl_env = DummyVecEnv([lambda: rl_env_instance]) # Wrap it

if os.path.isfile(rl_model_path):
    try:
        # rl_model = PPO.load(rl_model_path, env=rl_env) # ‡∏ñ‡πâ‡∏≤ PIMMAEnv ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å save/load ‡∏û‡∏£‡πâ‡∏≠‡∏° model
        rl_model = PPO.load(rl_model_path) # ‡∏ñ‡πâ‡∏≤ env ‡∏ñ‡∏π‡∏Å save ‡πÑ‡∏õ‡∏Å‡∏±‡∏ö model ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞ set ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á
        # ‡∏ñ‡πâ‡∏≤ env ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å load ‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢, ‡∏ï‡πâ‡∏≠‡∏á set ‡πÉ‡∏´‡∏°‡πà:
        if rl_model.get_env() is None:
            temp_env = PIMMAEnv(feature_dim)
            rl_model.set_env(DummyVecEnv([lambda: temp_env]))
        logging.info(f"[RL_SETUP] Loaded RL model from {rl_model_path}")
    except Exception as e:
        logging.error(f"[RL_SETUP] Error loading RL model from {rl_model_path}: {e}. Creating new model.", exc_info=True)
        temp_env = PIMMAEnv(feature_dim)
        rl_env_for_new_model = DummyVecEnv([lambda: temp_env])
        rl_model = PPO("MlpPolicy", rl_env_for_new_model, verbose=0, n_steps=128) # n_steps ‡πÄ‡∏õ‡πá‡∏ô hyperparam ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        rl_model.save(rl_model_path)
        logging.info(f"[RL_SETUP] Created and saved new RL model to {rl_model_path}")
else:
    temp_env = PIMMAEnv(feature_dim)
    rl_env_for_new_model = DummyVecEnv([lambda: temp_env])
    rl_model = PPO("MlpPolicy", rl_env_for_new_model, verbose=0, n_steps=128) # ‡∏õ‡∏£‡∏±‡∏ö n_steps ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    rl_model.save(rl_model_path)
    logging.info(f"[RL_SETUP] New RL model created and saved to {rl_model_path}")


# buffer ‡πÄ‡∏Å‡πá‡∏ö transitions
rl_buffer = []
buffer_lock = threading.Lock()

# === Model Loading ===
def load_model(path=MODEL_PATH):
    """Loads the XGBoost model from the specified path."""
    global model
    if os.path.exists(path) and not USE_FAKE_MODEL:
        try:
            model = joblib.load(path)
            logging.info(f"[MODEL STATUS] ‚úÖ Model loaded successfully from {path}.")
            return True
        except Exception as e:
            logging.error(f"[MODEL STATUS] ‚ùå Failed to load model from {path}", exc_info=True)
            model = None
            return False
    elif USE_FAKE_MODEL:
        logging.warning("[MODEL STATUS] ‚ö†Ô∏è Using FAKE model as per configuration.")
        model = None # Ensure model is None if fake is used, predict_entry will handle it
        return True # Faking is a valid state
    else:
        logging.warning(f"[MODEL STATUS] ‚ö†Ô∏è Model file not found at {path}. Waiting for retrain or WFT best model activation.")
        model = None
        return False
   
load_model() # Initial attempt to load the model

def load_model(path=MODEL_PATH):
    global model # 'explainer' ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢ _initialize_shap_explainer ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ä‡πâ global 'explainer'
    
    model_actually_loaded = False # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á

    if os.path.exists(path) and not USE_FAKE_MODEL:
        try:
            model = joblib.load(path)
            logging.info(f"[MODEL STATUS] ‚úÖ Real model loaded successfully from {path}.")
            model_actually_loaded = True
        except Exception as e:
            logging.error(f"[MODEL STATUS] ‚ùå Failed to load real model from {path}", exc_info=True)
            model = None # ‡∏´‡∏≤‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÉ‡∏´‡πâ model ‡πÄ‡∏õ‡πá‡∏ô None
            model_actually_loaded = False
            
    elif USE_FAKE_MODEL:
        logging.warning("[MODEL STATUS] ‚ö†Ô∏è Using FAKE model as per configuration.")
        model = None # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ FAKE model ‡∏Å‡πá‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÇ‡∏´‡∏•‡∏î
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ return ‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ "‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à" ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡πÉ‡∏ä‡πâ fake model
        # ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞ return ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ fake model ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö
        # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏´‡πâ model_actually_loaded ‡πÄ‡∏õ‡πá‡∏ô False ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á" ‡πÇ‡∏´‡∏•‡∏î
        model_actually_loaded = False # ‡∏´‡∏£‡∏∑‡∏≠ True ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ fake model ‡∏Ñ‡∏∑‡∏≠ "‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à" ‡πÅ‡∏ö‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á

    else: # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏û‡∏ö ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ fake model
        logging.warning(f"[MODEL STATUS] ‚ö†Ô∏è Model file not found at {path} and not using FAKE model.")
        model = None
        model_actually_loaded = False
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å _initialize_shap_explainer() ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ explainer ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á global 'model'
    _initialize_shap_explainer()
    
    # ‡∏Å‡∏≤‡∏£ return ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô load_model:
    # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞ return boolean ‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏£‡∏ß‡∏° fake model)"
    # ‡∏´‡∏£‡∏∑‡∏≠ "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠ fake) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà:
    if model is not None or USE_FAKE_MODEL:
        return True # ‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÇ‡∏´‡∏•‡∏î ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡πÉ‡∏ä‡πâ fake model
    return False # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î„É´‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡πÉ‡∏ä‡πâ fake model
    
# === Core AI Logic ===
def predict_entry_logic(features):
    """Core logic for making a prediction using the loaded model or a fake one."""
    if USE_FAKE_MODEL or model is None:
        logging.info("[PREDICT_LOGIC] Using fake model or model not loaded. Returning random prediction.")
        return round(np.random.uniform(0, 1), 4)
    try:
        # Ensure features is a list of floats
        if not isinstance(features, list) or not all(isinstance(f, (int, float)) for f in features):
            logging.error(f"[PREDICT_LOGIC] Invalid features format: {features}. Expected list of numbers.")
            # Fallback to fake prediction if features are malformed
            return round(np.random.uniform(0, 1), 4)

        prediction = model.predict(np.array([features]))[0] # XGBoost expects 2D array
        return float(prediction) # Ensure it's a Python float
    except Exception as e:
        logging.error("[PREDICT_LOGIC] Error during prediction", exc_info=True)
        # Fallback to fake prediction on error
        return round(np.random.uniform(0, 1), 4)

def _retrain_model_core(data_path=TRADE_DATA_LOG_FILE, output_model_path=MODEL_PATH):
    """Core logic for retraining the model."""
    start_time = time.time() # ‡∏¢‡πâ‡∏≤‡∏¢ start_time ‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å try ‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡πÉ‡∏ô try ‡∏Å‡πá‡πÑ‡∏î‡πâ
                           # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å try ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì duration ‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡πÄ‡∏Å‡∏¥‡∏î error ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏° try

    try: # <--- try block ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
        if not os.path.exists(data_path):
            _save_retrain_status("failed_no_file", 0, 0) # duration ‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°
            logging.error(f"[RETRAIN_CORE] Data file not found at {data_path}")
            return False, "Data file not found", 0, 0

        df = pd.read_csv(data_path)
        df = df[df['command'].str.upper() == "PREDICT_ENTRY"]
        df = df[df['features'].notna() & df['result'].notna() & (df['result'] != "")]
        
        try:
            df['result'] = df['result'].astype(float)
        except ValueError as e:
            duration_err = round(time.time() - start_time, 2)
            _save_retrain_status("failed_invalid_result_data", 0, duration_err)
            logging.error(f"[RETRAIN_CORE] Error converting 'result' to float: {e}. Check data in {data_path}.", exc_info=True)
            return False, "Invalid data in 'result' column", 0, duration_err

        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° X, y (‡∏™‡πà‡∏ß‡∏ô Robust X,y Preparation ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ) ---
        processed_data = []
        for index, row in df.iterrows():
            features = parse_feature_string(row['features'])
            if features:
                processed_data.append({'X': features, 'y': row['result']})
        
        if not processed_data:
            duration_err = round(time.time() - start_time, 2)
            _save_retrain_status("failed_no_valid_features", 0, duration_err)
            logging.error("[RETRAIN_CORE] No valid features data after parsing to train the model.")
            return False, "No valid features to train on", 0, duration_err

        feature_length = len(processed_data[0]['X'])
        X_final = []
        y_final = []
        for item in processed_data:
            if len(item['X']) == feature_length:
                X_final.append(item['X'])
                y_final.append(item['y'])
            else:
                logging.warning(f"[RETRAIN_CORE] Inconsistent feature length found. Expected {feature_length}, got {len(item['X'])}. Skipping row.")

        if not X_final:
            duration_err = round(time.time() - start_time, 2)
            _save_retrain_status("failed_no_consistent_features", 0, duration_err)
            logging.error("[RETRAIN_CORE] No features with consistent length to train the model.")
            return False, "No features with consistent length", 0, duration_err
        # --- ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î Robust X,y Preparation ---

        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Train model ‡πÉ‡∏´‡∏°‡πà ---
        model_new = xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)
        model_new.fit(np.array(X_final), np.array(y_final))
        joblib.dump(model_new, output_model_path)

        duration = round(time.time() - start_time, 2)
        _save_retrain_status("success", len(X_final), duration)
        logging.info(f"[RETRAIN_CORE] Model retrained successfully. Samples: {len(X_final)}, Duration: {duration}s. Saved to {output_model_path}")
        
        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï global model ‡πÅ‡∏•‡∏∞ SHAP explainer ---
        if output_model_path == MODEL_PATH:
            global model # ‡πÉ‡∏ä‡πâ global model ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÑ‡∏ß‡πâ‡∏ô‡∏≠‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
            model = model_new
            logging.info("[RETRAIN_CORE] Global model updated with newly retrained model.")
            _initialize_shap_explainer() # <--- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
            
        return True, "Retrain completed", len(X_final), duration

    except Exception as e: # <--- except block ‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö try ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
        # ‡∏´‡∏≤‡∏Å start_time ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡∏≠‡∏Å try block ‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì duration ‡πÑ‡∏î‡πâ
        # ‡∏´‡∏≤‡∏Å start_time ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô try ‡πÅ‡∏•‡∏∞ error ‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á start_time, duration ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å define
        # ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® duration_err ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏´‡∏£‡∏∑‡∏≠ ‡∏¢‡πâ‡∏≤‡∏¢ start_time ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
        current_duration_on_error = round(time.time() - start_time, 2) if 'start_time' in locals() else 0
        _save_retrain_status("failed_exception", 0, current_duration_on_error)
        logging.error("[RETRAIN_CORE] Exception during model retraining", exc_info=True)
        return False, str(e), 0, current_duration_on_error
        
# === Data Logging Functions ===
def log_trade_data(command, features_str, result=None, ticket=None, file_path=TRADE_DATA_LOG_FILE):
    """Logs trade-related data to a CSV file."""
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "command": str(command),
        "features": str(features_str), # Store as string
        "result": result if result is not None else "",
        "ticket": ticket if ticket is not None else ""
    }
    df_new_log = pd.DataFrame([log_entry])
    try:
        if not os.path.exists(file_path):
            df_new_log.to_csv(file_path, index=False)
        else:
            df_new_log.to_csv(file_path, mode="a", header=False, index=False)
    except IOError as e:
        logging.error(f"[LOG_TRADE_DATA] Error writing to {file_path}: {e}", exc_info=True)

def log_generic_csv(file_path, data_dict):
    """Logs a dictionary to a specified CSV file."""
    df_entry = pd.DataFrame([data_dict])
    try:
        if not os.path.exists(file_path):
            df_entry.to_csv(file_path, index=False)
        else:
            df_entry.to_csv(file_path, mode="a", header=False, index=False)
    except IOError as e:
        logging.error(f"[LOG_GENERIC_CSV] Error writing to {file_path}: {e}", exc_info=True)

# === License Key System ===
def validate_license(http_request):
    """Validates the license key from the request headers."""
    if not ENFORCE_LICENSE:
        return True
    license_key = http_request.headers.get("X-License-Key", "")
    return license_key in ALLOWED_LICENSE_KEYS

# === Flask Routes ===
@app.route("/download_csv", methods=["GET"])
def download_csv():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    try:
        return send_file(TRADE_DATA_LOG_FILE, as_attachment=True)
    except FileNotFoundError:
        logging.error(f"[DOWNLOAD_CSV] File not found: {TRADE_DATA_LOG_FILE}", exc_info=True)
        return jsonify({"error": "Trade data log file not found."}), 404
    except Exception as e:
        logging.error("[DOWNLOAD_CSV] Error", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/predict', methods=['POST'])
def predict_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    try:
        payload = request.get_json()
        if not payload or 'data' not in payload:
            return jsonify({"error": "Missing 'data' in payload"}), 400
        
        features_str = payload.get('data', "")
        features_list = parse_feature_string(features_str)

        if not features_list:
            logging.warning(f"[PREDICT_ROUTE] Received empty or invalid features string: {features_str}")
            return jsonify({"error": "Invalid or empty features string provided"}), 400

        logging.info(f"[PREDICT_ROUTE] Received features (sample): {features_list[:5]}‚Ä¶ total={len(features_list)}")
        
        ai_score = predict_entry_logic(features_list)
        
        logging.info(f"[PREDICT_ROUTE] AI Score: {ai_score:.4f}")
        log_trade_data("PREDICT_ENTRY", features_str, ai_score) # Log original string and score
        return jsonify(ai_score), 200 # ai_score is already float

    except Exception as e:
        logging.error("[PREDICT_ROUTE] Exception occurred", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/optimize', methods=['POST'])
def optimize_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    try:
        raw_data = request.get_data()
        json_str = raw_data.decode('utf-8')
        payload = json.loads(json_str)
        params = payload.get("current_params", payload)

        optimized = {}
        for k, v_str in params.items():
            try:
                v = float(v_str)
                new_val = v * np.random.uniform(0.9, 1.1)
                optimized[k] = round(new_val, 3) # Python float
            except ValueError:
                logging.warning(f"[OPTIMIZER] Could not convert param {k} value '{v_str}' to float. Skipping.")
                optimized[k] = v_str # Keep original if not convertible

        logging.info(f"[OPTIMIZER] Optimized result: {optimized}")
        return jsonify(optimized), 200
    except json.JSONDecodeError as e:
        logging.error("[OPTIMIZER] JSON decode error", exc_info=True)
        return jsonify({"error": f"Invalid JSON: {e}", "raw_data_sample": raw_data[:100].decode('utf-8', errors='replace')}), 400
    except Exception as e:
        logging.error("[OPTIMIZER] Error occurred", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/monitor', methods=['GET'])
def monitor_route():
    # No license check for basic monitoring usually
    try:
        global model
        model_status_str = "NOT_LOADED"
        if USE_FAKE_MODEL:
            model_status_str = "FAKE"
        elif model is not None:
            model_status_str = "REAL"
            
        info = {
            "model_status": model_status_str,
            "model_path_configured": MODEL_PATH,
            "model_file_exists": os.path.exists(MODEL_PATH),
            "latency_ms": int(np.random.randint(20, 80)), # Python int
            "enforce_license": ENFORCE_LICENSE,
            "timestamp": datetime.now().isoformat()
        }
        logging.info("[MONITOR] Health check requested.")
        return jsonify(info), 200
    except Exception as e:
        logging.error("[MONITOR] Exception", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route("/retrain_status", methods=["GET"])
def retrain_status_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    try:
        with open(LAST_RETRAIN_JSON, "r") as f:
            data = json.load(f)
        
        html = f"""
        <html><head><title>üß† Retrain Status</title></head><body>
            <h2>üß† AI Model Retrain Status</h2>
            <ul>
                <li><b>üìÖ Timestamp:</b> {data.get("timestamp", "-")}</li>
                <li><b>‚úÖ Samples Used:</b> {data.get("samples_used", "-")}</li>
                <li><b>‚è±Ô∏è Duration:</b> {data.get("duration_seconds", "-")} seconds</li>
                <li><b>üìà Status:</b> {data.get("status", "-")}</li>
            </ul>
            <p>
                <a href="/visualize_summary"><button>üìä View Profit Summary Chart</button></a>
                <a href="/wft_summary"><button>üìà View WFT Summary Chart</button></a>
            </p>
            <p>
                <a href="/download_csv"><button>‚¨á Download trade_data.csv</button></a>
                <a href="/download_report"><button>üìã Download Latest Report PDF</button></a>
            </p>
            <p><a href="/dashboard"><button>üè† Back to Dashboard</button></a></p>
        </body></html>"""
        return html
    except FileNotFoundError:
        logging.warning(f"[RETRAIN_STATUS] {LAST_RETRAIN_JSON} not found.")
        return f"<p style='color:orange;'>‚ö†Ô∏è Retrain status file not found. Run a retrain cycle first.</p><p><a href='/dashboard'>Back to Dashboard</a></p>", 404
    except Exception as e:
        logging.error("[RETRAIN_STATUS] Error", exc_info=True)
        return f"<p style='color:red;'>‚ùå Error loading retrain status: {e}</p>", 500

@app.route('/explain', methods=['POST'])
def explain_shap_route(): # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏° (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏•‡∏ö)
    global explainer
    if explainer is None:
        return jsonify({"error": "SHAP explainer not initialized. Model might not be loaded or compatible."}), 503 # Service Unavailable

    try:
        payload = request.get_json()
        if not payload or "features" not in payload:
            return jsonify({"error": "Missing 'features' in payload"}), 400

        feats_input = payload['features']
        if not isinstance(feats_input, list):
             return jsonify({"error": "'features' must be a list of numbers"}), 400
        
        # SHAP TreeExplainer ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á input ‡πÄ‡∏õ‡πá‡∏ô 2D array (n_samples, n_features)
        # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏£‡∏±‡∏ö features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1 instance
        try:
            feats = np.array(feats_input, dtype=np.float32).reshape(1, -1)
        except ValueError as ve:
            return jsonify({"error": f"Invalid feature format or type: {ve}"}), 400

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features ‡∏ñ‡πâ‡∏≤ model ‡∏°‡∏µ attribute ‡∏ô‡∏µ‡πâ (XGBoost model ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ n_features_in_)
        if hasattr(model, 'n_features_in_') and feats.shape[1] != model.n_features_in_:
            return jsonify({"error": f"Feature mismatch. Expected {model.n_features_in_} features, got {feats.shape[1]}."}), 400

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì SHAP values
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost regressor (single output), explainer.shap_values(feats) ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô NumPy array shape (1, n_features)
        shap_values_for_instance = explainer.shap_values(feats) # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô (1, n_features)

        # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ SHAP values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö instance ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å) ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô list
        # ‡πÅ‡∏•‡∏∞ base_value (‡∏´‡∏£‡∏∑‡∏≠ expected_value)
        if isinstance(shap_values_for_instance, list):
            # ‡∏Å‡∏£‡∏ì‡∏µ multi-output/multi-class (‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÇ‡∏ô‡πâ‡∏ï‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤ regression ‡πÉ‡∏ä‡πâ index [0])
            # ‡πÅ‡∏ï‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single output regression ‡∏à‡∏≤‡∏Å TreeExplainer ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà list
            # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°:
            shap_values_to_send = shap_values_for_instance[0].tolist()
            # expected_value ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö multi-class ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô array, ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö regression ‡πÄ‡∏õ‡πá‡∏ô float
            base_val = explainer.expected_value[0] if isinstance(explainer.expected_value, np.ndarray) else explainer.expected_value
        elif isinstance(shap_values_for_instance, np.ndarray) and shap_values_for_instance.ndim == 2:
            shap_values_to_send = shap_values_for_instance[0].tolist() # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å (instance ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
            base_val = explainer.expected_value # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö regression ‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô float ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î
            logging.error(f"[EXPLAIN_SHAP] Unexpected shap_values format: {type(shap_values_for_instance)}")
            return jsonify({"error": "Unexpected SHAP values format from explainer."}), 500

        return jsonify({
            "shap_values": shap_values_to_send,
            "base_value": float(base_val) # explainer.expected_value ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô float ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö regression
        })

    except Exception as e:
        logging.error(f"[EXPLAIN_SHAP] Error calculating SHAP values: {e}", exc_info=True)
        return jsonify({"error": f"Could not calculate SHAP values: {str(e)}"}), 500

@app.route('/summary', methods=['GET'])
def summary_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    try:
        if not os.path.exists(TRADE_DATA_LOG_FILE):
            return jsonify({"error": f"{TRADE_DATA_LOG_FILE} not found. No data to summarize."}), 404
            
        df = pd.read_csv(TRADE_DATA_LOG_FILE)
        # Filter for rows where 'result' is not NaN and not an empty string
        df = df[df['result'].notna() & (df['result'] != "")]
        
        # Attempt to convert 'result' to float, coercing errors to NaN, then drop NaN
        df['result'] = pd.to_numeric(df['result'], errors='coerce')
        df.dropna(subset=['result'], inplace=True)

        if df.empty:
            return jsonify({"message": "No valid trade results found to summarize."}), 200

        total_trades = len(df)
        avg_result = float(round(df['result'].mean(), 4)) if not df['result'].empty else 0.0
        std_dev_result = float(round(df['result'].std(), 4)) if not df['result'].empty else 0.0
        max_win = float(round(df['result'].max(), 4)) if not df['result'].empty else 0.0
        max_loss = float(round(df['result'].min(), 4)) if not df['result'].empty else 0.0
        # Ensure total_trades is not zero before division
        winrate = float(round((df['result'] > 0).sum() / total_trades * 100, 2)) if total_trades > 0 else 0.0

        stats = {
            "total_trades": total_trades,
            "average_result": avg_result,
            "std_deviation_result": std_dev_result,
            "max_win": max_win,
            "max_loss": max_loss,
            "winrate_percent": winrate,
            "last_updated": datetime.now().isoformat(),
            "data_source": TRADE_DATA_LOG_FILE
        }
        logging.info("[SUMMARY] Generated summary stats.")
        return jsonify(stats), 200
    except Exception as e:
        logging.error("[SUMMARY] Error generating summary", exc_info=True)
        return jsonify({"error": str(e)}), 500

def _generate_plot_base64(plot_function, *args, **kwargs):
    """Helper to generate a plot and return its base64 encoded string."""
    fig, ax = plt.subplots(figsize=(10, 5)) # Standardized figure size
    plot_function(ax, *args, **kwargs) # Call the specific plot drawing function
    
    buf = io.BytesIO()
    plt.savefig(buf, format="png", bbox_inches='tight')
    buf.seek(0)
    img64 = base64.b64encode(buf.read()).decode("utf-8")
    plt.close(fig) # Close the figure to free memory
    return img64

def _plot_cumulative_pnl(ax, pnl_series):
    """Plots cumulative P&L on a given Matplotlib Axes object."""
    ax.plot(np.cumsum(pnl_series), label="Cumulative Result", color='green')
    ax.set_title("AI Result Summary (Cumulative)")
    ax.set_xlabel("Number of Trades/Events")
    ax.set_ylabel("Cumulative Result Value")
    ax.grid(True)
    ax.legend()

@app.route('/visualize_summary')
def visualize_summary_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    error_html_template = """
        <html><head><title>AI Profit Summary</title></head><body>
            <h1>üìä AI Profit Summary</h1>
            <p style="color:{color};font-weight:bold;">{message}</p>
            <p><a href="/download_csv"><button>‚¨á Download trade_data.csv</button></a>
            <a href="/dashboard"><button>üè† Back to Dashboard</button></a></p>
        </body></html>"""
    try:
        if not os.path.exists(TRADE_DATA_LOG_FILE):
            return error_html_template.format(color="orange", message=f"‚ö†Ô∏è Data file '{TRADE_DATA_LOG_FILE}' not found."), 404

        df = pd.read_csv(TRADE_DATA_LOG_FILE)
        if "result" not in df.columns:
            return error_html_template.format(color="red", message="‚ùå 'result' column not found in data."), 200
        
        df['result'] = pd.to_numeric(df['result'], errors='coerce')
        pnl_series = df["result"].dropna()

        if len(pnl_series) < 2:
            return error_html_template.format(color="orange", message="‚ö†Ô∏è Not enough valid result data (at least 2 points required) to generate a chart."), 200

        img64 = _generate_plot_base64(_plot_cumulative_pnl, pnl_series)

        return f"""
        <html><head><title>AI Profit Summary</title></head><body>
            <h1>üìä AI Profit Summary</h1>
            <img src="data:image/png;base64,{img64}" alt="Summary Chart"/>
            <br/><br/>
            <p><a href="/download_csv"><button>‚¨á Download trade_data.csv</button></a>
            <a href="/dashboard"><button>üè† Back to Dashboard</button></a></p>
        </body></html>""", 200
    except Exception as e:
        logging.error("[VISUALIZE_SUMMARY] Error generating chart", exc_info=True)
        return error_html_template.format(color="red", message=f"‚ùå An error occurred: {e}"), 500

# --- Placeholder/Simple Data Routes ---
@app.route('/trends')
def trends_route():
    kw = request.args.get('kw', 'general_topic')
    # Placeholder: In a real app, query Google Trends API or similar
    return jsonify(keyword=kw, trend_value=round(np.random.uniform(30, 90), 1), source="Placeholder Trends API"), 200

@app.route('/social')
def social_route():
    src = request.args.get('src', 'twitter')
    # Placeholder: In a real app, query Twitter/Reddit API and perform sentiment analysis
    return jsonify(source_platform=src, sentiment_score=round(np.random.uniform(-0.8, 0.8), 2), analysis_type="Placeholder Sentiment"), 200

@app.route('/macro')
def macro_route():
    name = request.args.get('name', 'PMI')
    # Placeholder: In a real app, query a financial data API
    return jsonify(indicator_name=name, value=round(np.random.uniform(45, 55), 1), source="Placeholder Macro Data API"), 200

@app.route('/orderbook')
def orderbook_route():
    symbol = request.args.get('symbol', 'BTCUSD')
    # Placeholder: In a real app, connect to exchange API
    return jsonify(trading_symbol=symbol, imbalance_ratio=round(np.random.uniform(-0.5, 0.5), 3), source="Placeholder Orderbook API"), 200

@app.route('/onchain')
def onchain_route():
    metric = request.args.get('metric', 'active_addresses')
    sym = request.args.get('symbol', 'ETH')
    # Placeholder: In a real app, query on-chain data provider
    return jsonify(crypto_symbol=sym, onchain_metric=metric, value=int(np.random.randint(1000, 100000)), source="Placeholder On-chain API"), 200

@app.route("/cot", methods=["GET"])
def cot_route():
    logging.info("[COT] COT Data requested")
    # Placeholder for Commitment of Traders data
    data = {"asset": "Gold", "net_long_commercials": round(np.random.uniform(50, 85), 2), "source": "Placeholder COT"}
    return jsonify(data), 200

@app.route("/openinterest", methods=["GET"])
def openinterest_route():
    logging.info("[OI] Open Interest requested")
    # Placeholder for Open Interest
    oi_value = int(np.random.randint(50000, 200000))
    return jsonify({"symbol": "XAUUSD_Futures", "open_interest": oi_value, "source": "Placeholder OI"}), 200

@app.route("/news", methods=["GET"])
def news_route():
    logging.info("[NEWS] News requested")
    # Placeholder for News Impact
    news_data = {
        "event_name": "FOMC Meeting Minutes",
        "impact_level": "high" if np.random.rand() > 0.5 else "medium",
        "affected_pairs": ["USDJPY", "EURUSD", "XAUUSD"],
        "scheduled_time": (datetime.now() + pd.Timedelta(hours=np.random.randint(1,5))).isoformat(),
        "source": "Placeholder News API"
    }
    return jsonify(news_data), 200

@app.route("/correlation", methods=["GET"])
def correlation_route():
    logging.info("[CORRELATION] Correlation data requested")
    # Placeholder for Correlation Data
    correlation_data = {
        "target_asset": "XAUUSD",
        "correlations": {
            "DXY": round(np.random.uniform(-0.9, -0.2), 2),
            "US10Y_BondYield": round(np.random.uniform(-0.5, 0.5), 2),
            "OIL": round(np.random.uniform(-0.3, 0.3), 2)
        },
        "source": "Placeholder Correlation Engine"
    }
    return jsonify(correlation_data), 200

@app.route("/vwap", methods=["GET"])
def vwap_route():
    symbol = request.args.get('symbol', 'XAUUSD')
    logging.info(f"[VWAP] VWAP requested for {symbol}")
    data = {"symbol": symbol, "vwap": round(np.random.uniform(1900, 2100), 2), "timeframe": "1H", "source": "Placeholder VWAP"}
    return jsonify(data), 200

@app.route("/volumeprofile", methods=["GET"])
def volumeprofile_route():
    symbol = request.args.get('symbol', 'XAUUSD')
    logging.info(f"[VolumeProfile] Volume Profile requested for {symbol}")
    profile = {
        "symbol": symbol,
        "point_of_control": round(np.random.uniform(1950, 1970), 2),
        "high_volume_node": round(np.random.uniform(1920, 1980), 2),
        "low_volume_node": round(np.random.uniform(1850, 1900), 2),
        "timeframe": "Daily",
        "source": "Placeholder Volume Profile"
    }
    return jsonify(profile), 200

@app.route("/harmonics", methods=["GET"])
def harmonics_route():
    symbol = request.args.get('symbol', 'EURUSD')
    logging.info(f"[Harmonics] Harmonic Pattern scan requested for {symbol}")
    patterns = ["Gartley", "Bat", "Butterfly", "Crab", "Shark"]
    data = {
        "symbol": symbol,
        "pattern_detected": np.random.choice(patterns),
        "status": "developing" if np.random.rand() > 0.3 else "valid_entry_zone",
        "entry_zone_start": round(np.random.uniform(1.0500, 1.0600), 4),
        "entry_zone_end": round(np.random.uniform(1.0601, 1.0700), 4),
        "timeframe": "H4",
        "source": "Placeholder Harmonics Scanner"
    }
    return jsonify(data), 200

# === Retraining and Model Management Routes ===
@app.route('/retrain', methods=['GET']) # Can be POST if parameters are sent, GET for simple trigger
def retrain_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    logging.info("[RETRAIN_ROUTE] Manual retrain triggered.")
    
    # Run core logic in a thread to make the HTTP request return faster for manual triggers
    # For scheduled tasks, it runs in the scheduler's thread already.
    def _run_and_log():
        with app.app_context(): # Need app context if core logic uses Flask specific things (not much here)
             success, message, samples, duration = _retrain_model_core()
             # Further logging or actions after retrain if needed
             if success:
                 logging.info(f"[RETRAIN_ROUTE_THREAD] Retrain successful via route: {message}")
             else:
                 logging.error(f"[RETRAIN_ROUTE_THREAD] Retrain failed via route: {message}")

    thread = threading.Thread(target=_run_and_log)
    thread.start()
    
    return jsonify({
        "status": "triggered",
        "message": "Retrain process initiated in the background. Check /retrain_status for updates."
    }), 202


@app.route('/log_result', methods=['POST'])
def log_result_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    try:
        payload = request.get_json()
        ticket = payload.get("ticket")
        result_val = payload.get("result") # Renamed to avoid conflict with 'result' variable from list comprehension

        if ticket is None or result_val is None:
            return jsonify({"error": "Missing 'ticket' or 'result' in payload"}), 400

        try:
            ticket = int(ticket)
            result_val = float(result_val)
        except ValueError:
            return jsonify({"error": "Ticket must be an integer and result must be a float."}), 400

        # This logic updates the last PREDICT_ENTRY without a ticket.
        # It assumes that PREDICT_ENTRY is logged first, then its outcome (ticket, result) is logged later.
        if not os.path.exists(TRADE_DATA_LOG_FILE):
            # If file doesn't exist, we can't update. Log as a new entry perhaps?
            # Or, more simply, state that no prior entry exists to update.
            # For now, let's ensure the file can be created by log_trade_data if needed,
            # but updating requires a prior entry.
            log_trade_data("RESULT_LOGGED_NO_PRIOR_PREDICT", f"Ticket: {ticket}", result_val, ticket)
            return jsonify({"status": "logged_as_new", "message": "No prior predict entry to update, logged as new result."}), 201

        df = pd.read_csv(TRADE_DATA_LOG_FILE)
        updated = False
        # Iterate backwards to find the most recent PREDICT_ENTRY without a ticket
        for idx in range(len(df) - 1, -1, -1):
            # Ensure 'ticket' column exists and handle potential float NaNs from CSV read
            # Also ensure 'command' column exists
            if 'command' in df.columns and df.at[idx, "command"].upper() == "PREDICT_ENTRY":
                current_ticket = df.at[idx, "ticket"] if 'ticket' in df.columns else np.nan
                if pd.isna(current_ticket) or str(current_ticket).strip() == "":
                    df.loc[idx, "ticket"] = ticket
                    df.loc[idx, "result"] = result_val # Update result as well, might be P/L
                    updated = True
                    break
        
        if updated:
            df.to_csv(TRADE_DATA_LOG_FILE, index=False)
            logging.info(f"[LOG_RESULT] Successfully updated trade log for ticket {ticket} with result {result_val}.")
            return jsonify({"status": "success", "message": f"Result for ticket {ticket} logged."}), 200
        else:
            # If no suitable PREDICT_ENTRY was found to update, log this as a new "RESULT_ONLY" entry.
            log_trade_data("RESULT_LOGGED_ORPHANED", f"Ticket: {ticket}", result_val, ticket)
            logging.warning(f"[LOG_RESULT] No matching PREDICT_ENTRY found to update for ticket {ticket}. Logged as orphaned result.")
            return jsonify({"status": "not_found_or_already_logged", "message": "No unlogged PREDICT_ENTRY found to associate this result with, or it was already logged. Logged as new/orphaned."}), 202

    except Exception as e:
        logging.error("[LOG_RESULT] Exception occurred", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route("/log_execution_failure", methods=["POST"])
def log_execution_failure_route():
    # No license for internal logging typically
    raw_data = request.get_data()
    try:
        # Handle potential null terminators from MQL4/5
        cleaned_raw_data = raw_data.rstrip(b'\x00')
        json_str = cleaned_raw_data.decode('utf-8')
        payload = json.loads(json_str)
    except (UnicodeDecodeError, json.JSONDecodeError) as e:
        logging.error(f"[LOG_EXEC_FAIL] JSON parse/decode error: {e}. Raw (first 100 bytes): {raw_data[:100]!r}", exc_info=True)
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "error_type": "PayloadParseError",
            "details": str(e),
            "raw_payload_sample": raw_data[:200].decode('ascii', errors='replace') # Save a sample
        }
        log_generic_csv(EXECUTION_FAILURES_CSV, log_entry)
        return jsonify({"error": f"Invalid JSON or encoding: {e}", "raw_bytes_sample": list(raw_data[:64])}), 400

    ts = datetime.now().isoformat()
    log_data = {"timestamp": ts}
    log_file_target = None

    if "reason" in payload and "ticket" in payload: # More specific for execution failures
        log_file_target = EXECUTION_FAILURES_CSV
        log_data["ticket"] = payload.get("ticket", "")
        log_data["reason"] = payload.get("reason", "Unknown reason")
        log_data["details"] = payload.get("details", "") # Optional extra details
        logging.info(f"[LOG_EXEC_FAIL] Logging execution failure for ticket {log_data['ticket']}: {log_data['reason']}")
    elif "score" in payload and "method" in payload and "ticket" in payload: # For exit decisions
        log_file_target = EXIT_DECISIONS_CSV
        log_data["ticket"] = payload.get("ticket", "")
        log_data["score"] = payload.get("score", 0.0)
        log_data["method"] = payload.get("method", "Unknown method")
        logging.info(f"[LOG_EXIT_DECISION] Logging exit decision for ticket {log_data['ticket']}: Score {log_data['score']}, Method {log_data['method']}")
    else:
        logging.warning(f"[LOG_EXEC_FAIL] Unknown payload structure for logging: {payload}")
        # Log to a generic error log or a specific "malformed_payloads.csv"
        log_generic_csv("logs/malformed_failure_logs.csv", {"timestamp": ts, "payload_received": json.dumps(payload)})
        return jsonify({"error": "Unknown payload structure. Required fields: (ticket, reason) or (ticket, score, method).", "received_payload": payload}), 400

    log_generic_csv(log_file_target, log_data)
    return jsonify({"status": "ok", "message": "Log received."}), 200

# This route seems redundant if /log_execution_failure handles exit decisions.
# Kept for compatibility if MQL uses it directly, but logic is similar.
@app.route("/log_exit_decision", methods=["POST"])
def log_exit_decision_route_specific():
    # This acts as a more specific endpoint, essentially a subset of /log_execution_failure
    raw_bytes = request.get_data()
    logging.info(f"[LOG_EXIT_DECISION_SPECIFIC] Raw bytes: {raw_bytes!r}")
    try:
        s = raw_bytes.rstrip(b'\x00').decode('utf-8')
        payload = json.loads(s)
    except Exception as e:
        logging.error(f"[LOG_EXIT_DECISION_SPECIFIC] JSON parse error: {e}, raw string: {s!r}", exc_info=True)
        return jsonify({"error": f"Invalid JSON: {e}", "raw_string_content": s}), 400

    if not all(k in payload for k in ["ticket", "score", "method"]):
        return jsonify({"error": "Missing required fields: ticket, score, method", "received_payload": payload}), 400

    log_data = {
        "timestamp": datetime.now().isoformat(),
        "ticket": payload.get("ticket", ""),
        "score": payload.get("score", 0.0),
        "method": payload.get("method", "")
    }
    log_generic_csv(EXIT_DECISIONS_CSV, log_data)
    logging.info(f"[LOG_EXIT_DECISION_SPECIFIC] Logged: {log_data}")
    return jsonify({"status": "ok", "message": "Exit decision logged."}), 200


@app.route("/log_fail_entry", methods=["POST"])
def log_fail_entry_route():
    # No license for internal logging
    try:
        payload = request.get_json()
        if not payload: return jsonify({"error": "Empty payload"}), 400

        log_data = {
            "timestamp": datetime.now().isoformat(),
            "score": payload.get("score", 0.0),
            "features": str(payload.get("features", "")), # Store as string
            "reason": payload.get("reason", "Unknown reason")
        }
        log_generic_csv(AI_FAIL_ENTRY_CSV, log_data)
        logging.info(f"[FAIL_ENTRY_LOG] Logged AI entry failure: {log_data['reason']}")
        return jsonify({"status": "logged"}), 200
    except Exception as e:
        logging.error("[FAIL_ENTRY_LOG] Error logging failed entry", exc_info=True)
        return jsonify({"error": str(e)}), 500

# --- HTML View Routes for Logs ---
def _generate_html_table_page(csv_path, page_title, error_message_not_found):
    """Helper to generate an HTML page displaying a CSV file as a table."""
    try:
        if not os.path.exists(csv_path):
            raise FileNotFoundError(error_message_not_found)
        
        df = pd.read_csv(csv_path)
        # Sanitize column names for HTML display if necessary (e.g., replace underscores)
        # df.columns = [col.replace('_', ' ').title() for col in df.columns]
        html_table = df.to_html(classes="table table-striped table-hover", index=False, border=0, escape=True)

        return f"""
        <html>
        <head>
            <title>{page_title}</title>
            <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
            <style> body {{ padding: 20px; font-family: Arial, sans-serif; }} h1 {{ margin-bottom: 20px; }} </style>
        </head>
        <body>
            <h1>{page_title}</h1>
            {html_table if not df.empty else '<p>No data logged yet.</p>'}
            <br><p><a href="/dashboard" class="btn btn-primary">‚¨Ö Back to Dashboard</a></p>
        </body></html>"""
    except FileNotFoundError:
        logging.warning(f"[HTML_TABLE_VIEW] File not found: {csv_path}")
        return f"<p style='color:orange;'>‚ö†Ô∏è {error_message_not_found}</p><p><a href='/dashboard'>Back to Dashboard</a></p>", 404
    except Exception as e:
        logging.error(f"[HTML_TABLE_VIEW] Error loading log for {csv_path}", exc_info=True)
        return f"<p style='color:red;'>‚ùå Error loading log: {e}</p><p><a href='/dashboard'>Back to Dashboard</a></p>", 500

@app.route("/fail_log", methods=["GET"])
def fail_log_view_route():
    return _generate_html_table_page(EXECUTION_FAILURES_CSV, "‚ùå Execution Failure Log", "Execution failure log not found.")

@app.route("/exit_decision_log", methods=["GET"]) # New route for specific exit decision log
def exit_decision_log_view_route():
    return _generate_html_table_page(EXIT_DECISIONS_CSV, "üö™ Exit Decision Log", "Exit decision log not found.")

@app.route("/fail_entry_log", methods=["GET"])
def fail_entry_log_view_route():
    return _generate_html_table_page(AI_FAIL_ENTRY_CSV, "üö´ AI Entry Fail Log", f"{AI_FAIL_ENTRY_CSV} not found.")


# --- Walking Forward Test (WFT) ---
def _run_walking_forward_test_core(
    data_path=TRADE_DATA_LOG_FILE,
    output_path=WFT_RESULTS_CSV,
    train_window=300, # Number of samples
    test_window=50   # Number of samples
):
    try:
        logging.info("[WFT_CORE] üîÅ Starting Walking Forward Test")
        if not os.path.exists(data_path):
            logging.error(f"[WFT_CORE] Data file not found at {data_path}")
            return {"status": "error", "message": f"Data file not found: {data_path}"}

        df = pd.read_csv(data_path)
        df = df[df['command'].str.upper() == "PREDICT_ENTRY"]
        df = df[df['features'].notna() & df['result'].notna() & (df['result'] != "")]
        df['result'] = pd.to_numeric(df['result'], errors='coerce')
        df.dropna(subset=['result'], inplace=True)
        
        # Parse features and keep only valid ones along with their results
        parsed_data = []
        for index, row in df.iterrows():
            features = parse_feature_string(row['features'])
            timestamp = pd.to_datetime(row['timestamp'], errors='coerce') # Add timestamp parsing
            if features and pd.notna(timestamp):
                parsed_data.append({'X': features, 'y': row['result'], 'timestamp': timestamp})
        
        if not parsed_data:
            logging.error("[WFT_CORE] No valid feature/result pairs after parsing for WFT.")
            return {"status": "error", "message": "No valid feature/result data for WFT."}

        # Ensure all feature lists have consistent length
        expected_len = len(parsed_data[0]['X'])
        df_wft_data = pd.DataFrame([d for d in parsed_data if len(d['X']) == expected_len])

        if len(df_wft_data) < train_window + test_window:
            msg = f"Not enough consistent data for WFT (required: {train_window + test_window}, found: {len(df_wft_data)})"
            logging.error(f"[WFT_CORE] {msg}")
            return {"status": "error", "message": msg}
        
        df_wft_data = df_wft_data.sort_values("timestamp").reset_index(drop=True)

        wft_results_list = []
        total_rounds = (len(df_wft_data) - train_window) // test_window

        if total_rounds <= 0:
            msg = f"Not enough data for even one WFT round with train_window={train_window}, test_window={test_window}."
            logging.error(f"[WFT_CORE] {msg}")
            return {"status": "error", "message": msg}

        for i in range(total_rounds):
            train_start_idx = i * test_window
            train_end_idx = train_start_idx + train_window
            test_start_idx = train_end_idx
            test_end_idx = test_start_idx + test_window

            if test_end_idx > len(df_wft_data): # Ensure test data does not exceed bounds
                break 

            train_set = df_wft_data.iloc[train_start_idx:train_end_idx]
            test_set = df_wft_data.iloc[test_start_idx:test_end_idx]
            
            if train_set.empty or test_set.empty:
                logging.warning(f"[WFT_CORE] Skipping round {i+1} due to empty train/test set.")
                continue

            X_train = np.array(train_set['X'].tolist())
            y_train = np.array(train_set['y'].tolist())
            X_test = np.array(test_set['X'].tolist())
            # y_test_actual = np.array(test_set['y'].tolist()) # Actual results for evaluation

            wft_model = xgb.XGBRegressor(n_estimators=50, max_depth=4, random_state=i) # Use round for different random_state
            wft_model.fit(X_train, y_train)
            
            # Predictions are the model's output, treat these as the "simulated trade results/scores"
            # The 'result' column in trade_data.csv is the target for the regressor.
            # If this target is already a P&L or performance metric, then 'preds' are predicted P&L/metric.
            preds = wft_model.predict(X_test) 

            avg_pred_metric = np.mean(preds)
            std_pred_metric = np.std(preds)
            # "Win rate" based on predicted metric being positive (if metric > 0 is "win")
            winrate_on_pred = np.mean(preds > 0) * 100 if len(preds) > 0 else 0
            # Drawdown on the sequence of predicted metrics
            mdd_on_pred = np.min(np.cumsum(preds)) if len(preds) > 0 else 0

            round_result_data = {
                "round": i + 1,
                "train_start_ts": train_set['timestamp'].iloc[0].isoformat(),
                "test_start_ts": test_set['timestamp'].iloc[0].isoformat(),
                "avg_predicted_metric": round(avg_pred_metric, 4),
                "std_predicted_metric": round(std_pred_metric, 4),
                "winrate_on_predicted_metric_percent": round(winrate_on_pred, 2),
                "max_drawdown_on_predicted_metric": round(mdd_on_pred, 4),
                "samples_train": len(X_train),
                "samples_test": len(X_test)
            }
            wft_results_list.append(round_result_data)
            logging.info(f"[WFT_CORE] Round {i+1}/{total_rounds} complete: AvgPred={avg_pred_metric:.2f}, WinRatePred={winrate_on_pred:.1f}%")

        if not wft_results_list:
            logging.warning("[WFT_CORE] No WFT rounds were completed.")
            return {"status": "warning", "message": "WFT completed but no rounds generated results."}
            
        results_df = pd.DataFrame(wft_results_list)
        results_df.to_csv(output_path, index=False)
        logging.info(f"[WFT_CORE] ‚úÖ Walking Forward Test finished. Results saved to {output_path}")
        return {"status": "success", "message": f"WFT finished. Results saved to {output_path}", "rounds_completed": len(wft_results_list)}

    except Exception as e:
        logging.error("[WFT_CORE] ‚ùå Error in walking forward test", exc_info=True)
        return {"status": "error", "message": f"WFT failed: {str(e)}"}

@app.route("/wft", methods=["GET"]) # Or POST if params
def wft_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    logging.info("[WFT_ROUTE] WFT initiated via route.")
    # Run WFT in a background thread
    # Get params from request.args if needed, e.g., /wft?train_window=400
    train_win = request.args.get('train_window', 300, type=int)
    test_win = request.args.get('test_window', 50, type=int)

    thread = threading.Thread(target=_run_walking_forward_test_core, args=(TRADE_DATA_LOG_FILE, WFT_RESULTS_CSV, train_win, test_win))
    thread.daemon = True # Allow main program to exit even if threads are running
    thread.start()
    return jsonify({"status": "triggered", "message": f"WFT process initiated in background with train_window={train_win}, test_window={test_win}. Check logs or WFT summary page."}), 202

def _plot_wft_summary(ax, df_wft):
    """Plots WFT summary on a given Matplotlib Axes."""
    ax.plot(df_wft['round'], df_wft['avg_predicted_metric'], label='Avg Predicted Metric', marker='o')
    ax.plot(df_wft['round'], df_wft['winrate_on_predicted_metric_percent'], label='Winrate on Predicted Metric (%)', marker='x')
    ax.set_title("üìä Walking Forward Test (WFT) Summary")
    ax.set_xlabel("Test Round Number")
    ax.set_ylabel("Performance Metric Value")
    ax.grid(True)
    ax.legend()

@app.route("/wft_summary")
def wft_summary_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    error_html_template = """
        <html><head><title>WFT Summary</title></head><body>
            <h1>üìà WFT Summary</h1>
            <p style="color:{color};font-weight:bold;">{message}</p>
            <p><a href="/wft"><button>üîÅ Run WFT Now</button></a>
            <a href="/dashboard"><button>üè† Back to Dashboard</button></a></p>
        </body></html>"""
    try:
        if not os.path.exists(WFT_RESULTS_CSV):
            return error_html_template.format(color="orange", message=f"‚ö†Ô∏è WFT results file ('{WFT_RESULTS_CSV}') not found. Please run WFT first."), 404

        df_wft = pd.read_csv(WFT_RESULTS_CSV)
        if df_wft.empty or 'round' not in df_wft.columns or \
           'avg_predicted_metric' not in df_wft.columns or \
           'winrate_on_predicted_metric_percent' not in df_wft.columns:
            return error_html_template.format(color="red", message="‚ùå WFT results file is empty or has incorrect columns."), 200
        
        # Ensure data types for plotting
        df_wft['round'] = pd.to_numeric(df_wft['round'], errors='coerce')
        df_wft['avg_predicted_metric'] = pd.to_numeric(df_wft['avg_predicted_metric'], errors='coerce')
        df_wft['winrate_on_predicted_metric_percent'] = pd.to_numeric(df_wft['winrate_on_predicted_metric_percent'], errors='coerce')
        df_wft.dropna(subset=['round', 'avg_predicted_metric', 'winrate_on_predicted_metric_percent'], inplace=True)

        if len(df_wft) < 1 : # Need at least one point to plot meaningfully
             return error_html_template.format(color="orange", message=f"‚ö†Ô∏è Not enough valid data in WFT results to generate chart."), 200

        img64 = _generate_plot_base64(_plot_wft_summary, df_wft)
        
        return f"""
        <html><head><title>WFT Summary</title></head><body>
            <h1>üìà Walking Forward Test Summary</h1>
            <img src="data:image/png;base64,{img64}" alt="WFT Summary Chart"/>
            <br/><br/>
            <p><a href="/wft"><button>üîÅ Run WFT Again</button></a>
            <a href="/download_csv"><button>‚¨á trade_data.csv</button></a>
            <a href="/dashboard"><button>üè† Back to Dashboard</button></a></p>
        </body></html>""", 200
    except Exception as e:
        logging.error("[WFT_SUMMARY_ROUTE] Error generating WFT summary chart", exc_info=True)
        return error_html_template.format(color="red", message=f"‚ùå An error occurred: {e}"), 500

def _train_model_from_best_wft_round(
    wft_results_path=WFT_RESULTS_CSV,
    trade_data_path=TRADE_DATA_LOG_FILE,
    output_model_path=BEST_MODEL_PATH,
    optimize_metric='winrate_on_predicted_metric_percent' # or 'avg_predicted_metric'
):
    try:
        if not os.path.exists(wft_results_path):
            return False, f"WFT results file not found: {wft_results_path}"
        
        df_wft = pd.read_csv(wft_results_path)
        if df_wft.empty or optimize_metric not in df_wft.columns:
            return False, f"WFT results empty or missing optimizing metric '{optimize_metric}'."

        # Find the best round based on the chosen metric (higher is better)
        best_row = df_wft.loc[df_wft[optimize_metric].idxmax()]
        
        best_train_start_ts = pd.to_datetime(best_row['train_start_ts'])
        num_train_samples_in_best_round = int(best_row["samples_train"])

        # Load all trade data
        df_all_trades = pd.read_csv(trade_data_path)
        df_all_trades = df_all_trades[df_all_trades['command'].str.upper() == "PREDICT_ENTRY"]
        df_all_trades['result'] = pd.to_numeric(df_all_trades['result'], errors='coerce')
        df_all_trades.dropna(subset=['features', 'result'], inplace=True)
        df_all_trades['timestamp'] = pd.to_datetime(df_all_trades['timestamp'])
        df_all_trades.sort_values('timestamp', inplace=True)

        # Filter data that was used for training in the best WFT round
        # This requires careful matching of timestamps and feature parsing as done in WFT
        training_data_for_best_model_list = []
        for index, row in df_all_trades.iterrows():
            if row['timestamp'] >= best_train_start_ts:
                features = parse_feature_string(row['features'])
                if features:
                    training_data_for_best_model_list.append({'X': features, 'y': row['result']})
                if len(training_data_for_best_model_list) >= num_train_samples_in_best_round:
                    break # Collected enough samples corresponding to the best WFT training set

        if len(training_data_for_best_model_list) < num_train_samples_in_best_round :
            return False, f"Could not gather enough training samples ({len(training_data_for_best_model_list)} found, {num_train_samples_in_best_round} expected) for the best WFT round."

        df_train_best = pd.DataFrame(training_data_for_best_model_list)
        X_train_best = np.array(df_train_best['X'].tolist())
        y_train_best = np.array(df_train_best['y'].tolist())

        # Train the final model using these parameters
        final_best_model = xgb.XGBRegressor(n_estimators=50, max_depth=4, random_state=int(best_row['round']))
        final_best_model.fit(X_train_best, y_train_best)
        joblib.dump(final_best_model, output_model_path)

        msg = f"Best model trained from WFT round {best_row['round']} (Metric {optimize_metric}: {best_row[optimize_metric]:.2f}). Saved to {output_model_path}."
        logging.info(f"[WFT_BEST_MODEL] {msg}")
        return True, msg
    except Exception as e:
        logging.error("[WFT_BEST_MODEL] Error training best model from WFT", exc_info=True)
        return False, str(e)

@app.route("/activate_best_wft_model", methods=["GET"])
def activate_best_wft_model_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    global model # We will update the global model
    try:
        success, message = _train_model_from_best_wft_round()
        if not success:
            return jsonify({"status": "error", "message": f"Failed to train best model from WFT: {message}"}), 500

        # Load the newly trained best model as the active model
        if load_model(path=BEST_MODEL_PATH): # load_model updates global 'model'
            return jsonify({"status": "success", "message": f"Best model from WFT activated. {message}"}), 200
        else:
            return jsonify({"status": "error", "message": "Best model trained from WFT but failed to load it as active model."}), 500
            
    except Exception as e:
        logging.error("[ACTIVATE_BEST_WFT_MODEL] Failed to activate best WFT model", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500


# --- Report Generation ---
def _create_exit_report_placeholder(output_dir="."):
    """Creates a placeholder trading exit report (text file)."""
    try:
        filename_base = f"TradingReport_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        # Placeholder text file for now
        txt_filename = os.path.join(output_dir, f"{filename_base}.txt") # Changed to .txt for placeholder

        report_content = f"""
        Trading Exit Report
        Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        ----------------------------------------------------
        This is a placeholder for the Trading Exit Report.
        
        Summary of Trades:
        - (Data from {TRADE_DATA_LOG_FILE} or {EXIT_DECISIONS_CSV} would go here)
        - Total Trades Considered: ...
        - Profitable Exits: ...
        - Losing Exits: ...

        Key Metrics:
        - Average P&L per Exit: ...
        - Win Rate of Exits: ...
        - Largest Winning Exit: ...
        - Largest Losing Exit: ...

        Notes:
        - (Any specific observations or issues encountered)
        
        Future PDF implementations should include charts and more detailed tables.
        """
        with open(txt_filename, "w") as f:
            f.write(report_content)
        logging.info(f"[REPORT_GEN] ‚úÖ Placeholder report saved as {txt_filename}")
        return txt_filename
    except Exception as e:
        logging.error("[REPORT_GEN] ‚ùå Error generating placeholder report", exc_info=True)
        return None

@app.route("/download_report", methods=["GET"])
def download_report_route():
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    try:
        # Reports are currently text files, saved in the script's CWD
        reports_dir = os.path.abspath(".") # Assuming reports are in the root for now
        # Look for .txt reports, as PDF is placeholder
        files = [f for f in os.listdir(reports_dir) if f.startswith("TradingReport_") and f.endswith(".txt")]
        if not files:
            return "<p>‚ùå No reports (TradingReport_*.txt) found in the application directory.</p><p><a href='/dashboard'>Back to Dashboard</a></p>", 404

        latest_file = sorted(files, key=lambda f: os.path.getmtime(os.path.join(reports_dir, f)), reverse=True)[0]
        return send_file(os.path.join(reports_dir, latest_file), as_attachment=True)
    except Exception as e:
        logging.error("[DOWNLOAD_REPORT] Error finding/sending report", exc_info=True)
        return f"<p style='color:red;'>‚ùå Error loading report: {e}</p>", 500

# --- Presets ---
@app.route("/preset/<name>")
def get_preset_route(name):
    if not validate_license(request): return jsonify({"error": "Invalid license"}), 403
    try:
        # Sanitize name to prevent directory traversal
        safe_name = "".join(c for c in name if c.isalnum() or c in ('_', '-'))
        if not safe_name:
            return jsonify({"error": "Invalid preset name"}), 400
            
        preset_file_path = os.path.join("./presets/", f"{safe_name}.json")
        with open(preset_file_path, "r") as f:
            preset_data = json.load(f)
        return jsonify(preset_data)
    except FileNotFoundError:
        logging.warning(f"[PRESET] Preset file not found: presets/{safe_name}.json")
        return jsonify({"error": f"Preset '{safe_name}' not found"}), 404
    except json.JSONDecodeError:
        logging.error(f"[PRESET] Error decoding JSON for preset: presets/{safe_name}.json", exc_info=True)
        return jsonify({"error": f"Preset '{safe_name}' is not valid JSON"}), 500
    except Exception as e:
        logging.error(f"[PRESET] Error loading preset '{name}'", exc_info=True)
        return jsonify({"error": f"Could not load preset: {e}"}), 500

@app.route('/drift', methods=['POST'])
def drift():
    global baseline_mean # ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ global baseline_mean
    try:
        if baseline_mean is None:
            # ‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î baseline_mean ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏ï‡∏≠‡∏ô train model
            # ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏° app ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥
            # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: baseline_mean = np.load('models/baseline_feature_mean.npy')
            # ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á return error ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤ default
            logging.error("[DRIFT] baseline_mean is not initialized.")
            return "Error: Baseline mean not initialized on server.", 500, {'Content-Type':'text/plain'}

        payload = request.get_json()
        if not payload or 'features' not in payload:
            return "Error: Missing 'features' in JSON payload.", 400, {'Content-Type':'text/plain'}

        feats_input = payload['features']
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ feats_input ‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not isinstance(feats_input, list) or not all(isinstance(x, (int, float)) for x in feats_input):
            return "Error: 'features' must be a list of numbers.", 400, {'Content-Type':'text/plain'}

        feats = np.array(feats_input, dtype=float)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö baseline_mean ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if feats.shape[0] != baseline_mean.shape[0]:
            logging.warning(f"[DRIFT] Feature length mismatch. Input: {feats.shape[0]}, Baseline: {baseline_mean.shape[0]}")
            return f"Error: Feature length mismatch. Expected {baseline_mean.shape[0]} features.", 400, {'Content-Type':'text/plain'}
        
        # Handle division by zero if len(feats) is 0 (though previous checks should prevent this)
        if len(feats) == 0:
            return "Error: Features array cannot be empty.", 400, {'Content-Type':'text/plain'}

        drift_value = np.linalg.norm(feats - baseline_mean) / len(feats)
        logging.info(f"[DRIFT] Calculated drift: {drift_value}")
        return str(float(drift_value)), 200, {'Content-Type':'text/plain'}

    except Exception as e:
        logging.error(f"[DRIFT] Error calculating drift: {e}", exc_info=True)
        return f"Error: Could not calculate drift - {str(e)}", 500, {'Content-Type':'text/plain'}

@app.route('/rl/predict', methods=['POST'])
def rl_predict():
    global rl_model # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ rl_model ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß
    if rl_model is None:
        return jsonify(error="RL model not loaded"), 500
    try:
        payload = request.get_json()
        if not payload or "features" not in payload:
            return jsonify(error="Missing 'features' in payload"), 400
        
        data = payload["features"]
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ data ‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feature_dim
        if not isinstance(data, list) or len(data) != feature_dim:
            return jsonify(error=f"Features must be a list of {feature_dim} numbers"), 400

        obs = np.array(data, dtype=np.float32).reshape(1, -1) # PPO ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á obs ‡∏ó‡∏µ‡πà‡∏°‡∏µ batch dimension
        action, _ = rl_model.predict(obs, deterministic=True)
        return jsonify(action=int(action))
    except Exception as e:
        logging.error(f"[RL_PREDICT] Error: {e}", exc_info=True)
        return jsonify(error=str(e)), 500


@app.route('/rl/store', methods=['POST'])
def rl_store():
    global rl_buffer, buffer_lock
    try:
        payload = request.get_json()
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö payload ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {"obs": [...], "action":0, "reward":0.5, "next_obs":[...], "done":False}
        required_keys = ["obs", "action", "reward", "next_obs", "done"]
        if not payload or not all(key in payload for key in required_keys):
            return jsonify(error=f"Missing one or more required keys: {required_keys}"), 400
        
        # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£ validate data types ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà

        with buffer_lock:
            rl_buffer.append(payload)
        # logging.info(f"[RL_STORE] Stored transition. Buffer size: {len(rl_buffer)}") # Optional: log buffer size
        return jsonify(status="stored", buffer_size=len(rl_buffer))
    except Exception as e:
        logging.error(f"[RL_STORE] Error: {e}", exc_info=True)
        return jsonify(error=str(e)), 500

@app.route('/rl/update', methods=['POST'])
def rl_update():
    global rl_model, rl_buffer, buffer_lock, rl_model_path
    if rl_model is None:
        return jsonify(error="RL model not loaded"), 500

    # ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ RL update ‡πÉ‡∏ô background thread ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ request ‡∏Ñ‡πâ‡∏≤‡∏á‡∏ô‡∏≤‡∏ô
    # ‡πÅ‡∏ï‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö synchronous ‡∏Å‡πà‡∏≠‡∏ô

    transitions_to_learn = []
    with buffer_lock:
        if not rl_buffer:
            return jsonify(status="no_new_data", message="No new transitions in buffer to update model."), 200
        transitions_to_learn = rl_buffer.copy()
        rl_buffer.clear()
    
    logging.info(f"[RL_UPDATE] Starting RL model update with {len(transitions_to_learn)} transitions.")

    # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö PPO ---
    # PPO ‡πÄ‡∏õ‡πá‡∏ô on-policy algorithm ‡∏õ‡∏Å‡∏ï‡∏¥‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å rollouts ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢ policy ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    # ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å buffer ‡∏Ç‡∏≠‡∏á (s, a, r, s', d) ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ (offline data) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PPO ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤ off-policy algorithms
    # `rl_model.learn(total_timesteps=X)` ‡∏à‡∏∞‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• run environment ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡πÄ‡∏≠‡∏á X timesteps
    # ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• obs, acts, rews ‡∏à‡∏≤‡∏Å buffer ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÉ‡∏ô PPO.learn() ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ

    # **‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ (‡πÅ‡∏ï‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤):**
    # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Replay Buffer ‡∏´‡∏£‡∏∑‡∏≠ Dataset ‡∏à‡∏≤‡∏Å `transitions_to_learn`.
    # 2. ‡∏õ‡∏£‡∏±‡∏ö PPO algorithm ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ algorithm ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö offline learning ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å buffer ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ
    # 3. ‡∏´‡∏£‡∏∑‡∏≠ `PIMMAEnv` ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ `step()` ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å `transitions_to_learn` ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ simulate ‡πÄ‡∏≠‡∏á
    #    (‡πÄ‡∏ä‡πà‡∏ô MQL ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÉ‡∏´‡πâ PIMMAEnv ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô step ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)

    # **‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ `rl_model.learn(total_timesteps=len(data))`**
    # ‡∏ô‡∏µ‡πà‡∏à‡∏∞‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ PPO model ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô run `PIMMAEnv` ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô `len(transitions_to_learn)` steps
    # ‡∏ã‡∏∂‡πà‡∏á `PIMMAEnv` ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏µ dummy `step` function.
    # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ" ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡πÜ `PIMMAEnv` ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ
    # ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ pre-training ‡∏î‡πâ‡∏ß‡∏¢ behavioral cloning ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏≠‡∏∑‡πà‡∏ô‡∏à‡∏≤‡∏Å buffer ‡∏ô‡∏µ‡πâ‡∏Å‡πà‡∏≠‡∏ô
    # ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ fine-tune ‡∏î‡πâ‡∏ß‡∏¢ PPO.learn() ‡πÅ‡∏ö‡∏ö online.

    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏°‡∏≤ (‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á logic ‡∏Å‡∏≤‡∏£ train ‡∏Ç‡∏≠‡∏á RL ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï):
    try:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ env ‡∏Ç‡∏≠‡∏á rl_model ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß
        if rl_model.get_env() is None:
            logging.warning("[RL_UPDATE] RL model environment not set. Setting a default one.")
            temp_env = PIMMAEnv(feature_dim)
            rl_model.set_env(DummyVecEnv([lambda: temp_env]))

        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô timesteps ‡∏ó‡∏µ‡πà‡∏à‡∏∞ train ‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ
        # ‡∏Å‡∏≤‡∏£ train PPO ‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô timesteps ‡∏ô‡πâ‡∏≠‡∏¢‡πÜ ‡∏ã‡πâ‡∏≥‡πÜ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÄ‡∏ó‡πà‡∏≤ train ‡∏î‡πâ‡∏ß‡∏¢ batch ‡πÉ‡∏´‡∏ç‡πà
        # ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö n_steps ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á PPO model
        # ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤ n_steps=128, total_timesteps ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô ŸÖÿ∂ÿßÿπŸÅÿßÿ™ ‡∏Ç‡∏≠‡∏á 128
        # ‡πÅ‡∏ï‡πà len(transitions_to_learn) ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô transitions ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏°‡∏≤
        # ‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£ train ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        num_timesteps_to_learn = max(128, len(transitions_to_learn)) # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ train 1 rollout (n_steps) ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
        
        logging.info(f"[RL_UPDATE] Calling rl_model.learn() with total_timesteps={num_timesteps_to_learn}")
        rl_model.learn(total_timesteps=num_timesteps_to_learn, reset_num_timesteps=False) # reset_num_timesteps=False ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏ö‡∏ï‡πà‡∏≠
        rl_model.save(rl_model_path)
        logging.info(f"[RL_UPDATE] RL model updated and saved to {rl_model_path}")
        return jsonify(status="rl_model_updated", transitions_processed=len(transitions_to_learn))
    except Exception as e:
        logging.error(f"[RL_UPDATE] Error during RL model update: {e}", exc_info=True)
        # ‡∏Ñ‡∏∑‡∏ô transitions ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ buffer ‡∏ñ‡πâ‡∏≤‡∏Å‡∏≤‡∏£ train ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡∏Ñ‡∏ß‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤)
        # with buffer_lock:
        #     rl_buffer.extend(transitions_to_learn) # Caution: could lead to repeated failures
        return jsonify(error=f"Failed to update RL model: {str(e)}"), 500

# --- Dashboard ---
@app.route("/dashboard")
def dashboard_route():
    # Basic dashboard, no license check needed for view generally
    return """
    <html>
    <head>
        <title>üìä AI Trading Dashboard</title>
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
        <style>
            body { padding: 20px; font-family: Arial, sans-serif; }
            .task-btn { margin-bottom: 10px; min-width: 250px; }
            .container { max-width: 800px; }
            h1, h2 { margin-bottom: 20px; }
            .list-group-item a { text-decoration: none; }
            .list-group-item button { width: 100%; text-align: left; }
        </style>
        <script>
        async function runTask(path, btnId, method = 'GET', body = null) {
            const btn = document.getElementById(btnId);
            const originalText = btn.innerText;
            btn.disabled = true;
            btn.innerHTML = '<span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Processing...';
            let message = '';
            try {
                const options = { method: method };
                if (method === 'POST' && body) {
                    options.headers = {'Content-Type': 'application/json'};
                    options.body = JSON.stringify(body);
                }
                const res = await fetch(path, options);
                const data = await res.json(); // Expect JSON response
                if (res.ok) {
                    message = `‚úÖ Success: ${data.message || JSON.stringify(data)}`;
                } else {
                    message = `‚ùå Error ${res.status}: ${data.error || JSON.stringify(data)}`;
                }
            } catch(e) {
                message = `‚ùå Network/Script Error: ${e}`;
            }
            alert(message); // Simple feedback
            btn.disabled = false;
            btn.innerText = originalText;
        }

        function viewPage(path) { window.location.href = path; }
        </script>
    </head>
    <body>
        <div class="container">
            <h1>üìä AI Trading Dashboard</h1>

            <h2>üöÄ Actions</h2>
            <div class="list-group mb-4">
                <button id="retrain-btn" class="list-group-item list-group-item-action task-btn btn btn-primary"
                        onclick="runTask('/retrain','retrain-btn')">
                    üîÅ Trigger Model Retrain
                </button>
                <button id="wft-btn" class="list-group-item list-group-item-action task-btn btn btn-info"
                        onclick="runTask('/wft','wft-btn')">
                    üìà Trigger Walking Forward Test (WFT)
                </button>
                <button id="activate-wft-btn" class="list-group-item list-group-item-action task-btn btn btn-success"
                        onclick="runTask('/activate_best_wft_model','activate-wft-btn')">
                    üåü Activate Best Model from WFT
                </button>
            </div>

            <h2>üìà Status & Visualizations</h2>
            <div class="list-group mb-4">
                <button class="list-group-item list-group-item-action task-btn btn btn-light" onclick="viewPage('/retrain_status')">üß† View Retrain Status</button>
                <button class="list-group-item list-group-item-action task-btn btn btn-light" onclick="viewPage('/summary')">üíπ View Trade Summary (JSON)</button>
                <button class="list-group-item list-group-item-action task-btn btn btn-light" onclick="viewPage('/visualize_summary')">üìä View Profit Summary Chart</button>
                <button class="list-group-item list-group-item-action task-btn btn btn-light" onclick="viewPage('/wft_summary')">üìà View WFT Summary Chart</button>
                <button class="list-group-item list-group-item-action task-btn btn btn-light" onclick="runTask('/monitor','monitor-btn-silent')">ü©∫ Check API Monitor (Alert)</button>
                 <button class="list-group-item list-group-item-action task-btn btn btn-light" onclick="viewPage('/explain')">üí° View Model Feature Importance (JSON)</button>
            </div>
            
            <h2>üìÑ Logs & Reports</h2>
            <div class="list-group mb-4">
                <button class="list-group-item list-group-item-action task-btn btn btn-light" onclick="viewPage('/download_csv')">‚¨á Download Trade Data CSV</button>
                <button class="list-group-item list-group-item-action task-btn btn btn-light" onclick="viewPage('/download_report')">üìã Download Latest Report</button>
                <button class="list-group-item list-group-item-action task-btn btn btn-warning" onclick="viewPage('/fail_log')">‚ùå View Execution Failure Log</button>
                <button class="list-group-item list-group-item-action task-btn btn btn-warning" onclick="viewPage('/exit_decision_log')">üö™ View Exit Decision Log</button>
                <button class="list-group-item list-group-item-action task-btn btn btn-warning" onclick="viewPage('/fail_entry_log')">üö´ View AI Entry Fail Log</button>
            </div>
        </div>
    </body></html>"""


# === Background Scheduler ===
# --- Scheduled Tasks ---
def scheduled_model_retrain_task():
    logging.info("[SCHEDULER] Initiating scheduled model retrain...")
    with app.app_context(): # Ensure Flask app context is available if needed by core logic
        success, message, samples, duration = _retrain_model_core()
    if success:
        logging.info(f"[SCHEDULER] ‚úÖ Scheduled model retrain completed. Samples: {samples}, Duration: {duration}s. Message: {message}")
    else:
        logging.error(f"[SCHEDULER] ‚ùå Scheduled model retrain failed. Message: {message}")

def scheduled_report_generation_task():
    logging.info("[SCHEDULER] Initiating scheduled report generation...")
    with app.app_context():
        report_file = _create_exit_report_placeholder() # Using placeholder
    if report_file:
        logging.info(f"[SCHEDULER] ‚úÖ Scheduled report generated: {report_file}")
    else:
        logging.error("[SCHEDULER] ‚ùå Scheduled report generation failed.")

# --- Scheduler Runner ---
def run_scheduler():
    # Define schedules
    # Example: Retrain every day at 1:00 AM server time
    schedule.every().day.at("01:00").do(scheduled_model_retrain_task)
    # Example: Generate report every day at 00:00 server time
    schedule.every().day.at("00:00").do(scheduled_report_generation_task)
    
    # Example: Run WFT weekly on Sunday at 03:00 AM
    # schedule.every().sunday.at("03:00").do(
    #    lambda: _run_walking_forward_test_core(TRADE_DATA_LOG_FILE, WFT_RESULTS_CSV) # Use lambda for args
    # )

    logging.info("[SCHEDULER] Scheduler started with defined tasks.")
    while True:
        schedule.run_pending()
        time.sleep(60) # Check every minute

# === Main Execution ===
if __name__ == '__main__':
    logging.info("[SERVER_INIT] Flask API service is starting...")
    print("[SERVER_INIT] Flask API service is starting...")

    # Start the background scheduler thread (ONLY ONCE)
    # Ensure this block does not run again if Flask's reloader is active (use_reloader=False handles this)
    if not (app.debug and os.environ.get('WERKZEUG_RUN_MAIN') == 'true'): # Check if not in reloader's subprocess
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        logging.info("[SERVER_INIT] Background scheduler thread started.")
        print("[SERVER_INIT] Background scheduler thread started.")
    else:
        logging.info("[SERVER_INIT] Scheduler thread already started by main Werkzeug process or skipped in reloader.")
        print("[SERVER_INIT] Scheduler thread already started by main Werkzeug process or skipped in reloader.")

    # Log server start
    print(f"[SERVER_INIT] Flask API is attempting to run on http://127.0.0.1:5000")
    logging.info(f"[SERVER_INIT] Flask API initialized, attempting to run on http://127.0.0.1:5000")

    # Run Flask app
    # use_reloader=False is important if you manage threads yourself to avoid them starting twice
    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)